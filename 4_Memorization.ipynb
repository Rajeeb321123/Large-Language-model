{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/RYH4E1u8fjdjmJL6kW6W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rajeeb321123/Large-Language-model/blob/master/4_Memorization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Packages"
      ],
      "metadata": {
        "id": "mBCzk8VphDRg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGDnnr5HeBEc"
      },
      "outputs": [],
      "source": [
        "!pip -m pip install --upgrade pip -1\n",
        "!pip install transformers == 4.38.1 -q -U\n",
        "!pip install bitsandbytes == 0.42.0 -q -U\n",
        "!pip install peft == 0.8.2 -q -U\n",
        "!pip install accelerate == 0.27.2 -q -U\n",
        "!pip install flash ==  -q -U\n",
        "!pip install  datasets\n",
        "!pip install  scipy\n",
        "!pip install  trl\n",
        "!pip install  hf_transfer\n",
        "!pip install  huggingface_hub\n",
        "!pip install  wanddb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!transformers-cli env"
      ],
      "metadata": {
        "id": "3uiIbKKWgdx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Unsloth install"
      ],
      "metadata": {
        "id": "AZ0EMjVOgvHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Model"
      ],
      "metadata": {
        "id": "AwXcOFXfg-r_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For gated models on HuggingFace\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "rLVIFd9lhGNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%env HF_HUB_ENABLE_HF_TRANSFER = True # for high speed downloading and uploading to hugging face hub"
      ],
      "metadata": {
        "id": "lrrQWjPmhRsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cache_dir = '' # comment out if Google Drive is aset as cache_dir\n",
        "\n",
        "# base model (Unsupervised Trial)\n",
        "model_id = \"openchat/openchat_3.5\""
      ],
      "metadata": {
        "id": "knZB6c-phmPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Load the model and Tokenizer of LoRA or DoRA\n",
        "from Transformers import AutoTokenizer, AutoModelForCausalllm, BitsAndBytes\n",
        "import torch\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit = True,\n",
        "    bnb_4bit_use_double_quant = True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16, # if newer gpu: bfloat16\n",
        ")"
      ],
      "metadata": {
        "id": "q7WvkXooiRhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# config = AutoConfig.from_pretrained(model_id)\n",
        "# cofig.max_position_embeddings = 4096 # (input + output) #model will only learn from max 4096 sequence of token"
      ],
      "metadata": {
        "id": "_TDi7OpEfYcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    #config=config,\n",
        "\n",
        "    #quantization_config=bnb_config,\n",
        "\n",
        "    #rope_scaling={\"type\":linear, \"factor\": 2.0}, # roPE scaling: https://www.hopsworks.ai/dictionary/rope-scaling and https://huggingface.co/docs/text-generation-inference/en/basic_tutorials/preparing_model\n",
        "\n",
        "    #https://huggingface.co/docs/accelerate/v0.25.0/en/concept_guides/big_model_inference\n",
        "    device_map='auto', # It’s fully possible to create your own device map for the layers to use as well, specifying the GPU device to use (a number), \"cpu\", or \"disk\" and pass this in:\n",
        "\n",
        "    # Here, the \"trust_remote_code=True\" means \"download the model code from huggingface repo 'internlm/internlm-chat-7b'\", along with the weight, and run it. If it's False, the library would use builtin model architectures hardcoded in huggingface/transformers and only download the weight.\n",
        "    #trust_remote_code=False,\n",
        "\n",
        "    torch_dtype=torch.float16, # if newer gpu: bfloat16\n",
        "\n",
        "    # https://huggingface.co/docs/text-generation-inference/en/conceptual/flash_attention\n",
        "    attn_implementation=\"flash_attention_2\" # Works with llama model\n",
        "\n",
        "    cache_dir = cache_dir\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True, trust_remote_code=False)"
      ],
      "metadata": {
        "id": "2WyrKD_WhXFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Load the Model and Tokenizer for Unsloth"
      ],
      "metadata": {
        "id": "bkm8areWsylC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading checks"
      ],
      "metadata": {
        "id": "Tdt9Gof7s39U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check there are no parameter overflowing onto cpu (meta)\n",
        "# Making sure all of the parameter are in GPU not in CPU\n",
        "for n, p in model.named_parameters():\n",
        "  if p.device.type == \"meta\":\n",
        "    print(f\"{n} is on meta\")"
      ],
      "metadata": {
        "id": "u6h6k1m5s7yT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.config.max_position_embeddings)\n",
        "\n",
        "#eos = end of sequence\n",
        "# https://huggingface.co/docs/transformers/en/pad_truncation\n",
        "# very important for pad and eos use: https://www.natebrake.com/blog/llm/end-of-sequence-explained\n",
        "print(model.congit.eso_token_id)"
      ],
      "metadata": {
        "id": "vrO5Z-q8tKbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Prepare for LoRA fine-tuning\n",
        "def print_trainable_parameters(model):\n",
        "  \"\"\"\n",
        "  Print the number of trainable parameters in the model and lists whic\n",
        "  \"\"\"\n",
        "  trainable_params = 0\n",
        "  non_trainable_params = 0\n",
        "  all_params = 0\n",
        "\n",
        "  print(\"Trainable Parameters:\")\n",
        "  for name, param in model.named_parameters():\n",
        "    # https://www.geeksforgeeks.org/python-pytorch-numel-method/\n",
        "    # Total no of all parameters (trainable + non trainable)\n",
        "    all_params += param.numel() #PyTorch torch.numel() method returns the total number of elements in the input tensor.\n",
        "\n",
        "    # source: copilot: ask about param.requires_drad\n",
        "    # When requires_grad is set to True, it indicates that the parameter participates in gradient computation during backpropagation (i.e., it’s trainable).\n",
        "    #When requires_grad is set to False, the parameter is excluded from gradient updates during training (i.e., it’s frozen).\n",
        "    if param.requires_grad:\n",
        "      trainable_params += param.numel()\n",
        "      print(f\"  {name} \")\n",
        "    else:\n",
        "      non_trainable_params += param.numel()\n",
        "\n",
        "  # This part is same as else portion above but just for printing we did it again\n",
        "  print(\"\\nNon_Trainable Parameters\")\n",
        "  for name, param in model.named_parameters():\n",
        "    if not param.requires_grad:\n",
        "      print(f\" {name} \")\n",
        "\n",
        "\n",
        "  print(\n",
        "      f\"\\nSummary:\\n Trainable params: {trainable_params}\\n Non-Trainable params:{non_trainable_params}\"\n",
        "  )"
      ],
      "metadata": {
        "id": "TkXPSLszUdCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Standard LoRA or DoRA"
      ],
      "metadata": {
        "id": "Y3P2OKHsYsx1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OeUh4M73YziD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}