{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rajeeb321123/Large-Language-model/blob/master/4_Memorization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBCzk8VphDRg"
      },
      "source": [
        "## Packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLV0BaUDkYvr"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owKD416rp_rb",
        "outputId": "bb72a0f4-f6cf-412c-c96c-33143ec1d75d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/2.1 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.8/2.1 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m1.7/2.1 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.2/251.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.2/245.2 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.4/102.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.1/281.1 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!python -m pip install --upgrade pip -q\n",
        "!pip install transformers  -q -U\n",
        "!pip install bitsandbytes  -q -U\n",
        "!pip install peft  -q -U\n",
        "!pip install accelerate  -q -U\n",
        "!pip install flash  -q -U\n",
        "!pip install  datasets -q -U\n",
        "!pip install  scipy -q -U\n",
        "!pip install  trl -q -U\n",
        "!pip install  hf_transfer -q -U\n",
        "!pip install  huggingface_hub -q -U\n",
        "!pip install  wandb -q -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uiIbKKWgdx2",
        "outputId": "d127fbc8-87b1-4966-8023-acd2542167d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-05-16 14:44:41.655662: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-16 14:44:41.655722: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-16 14:44:41.788451: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-05-16 14:44:43.995002: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/transformers/commands/env.py:100: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n",
            "2024-05-16 14:44:50.541134: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "\n",
            "Copy-and-paste the text below in your GitHub issue and FILL OUT the two last points.\n",
            "\n",
            "- `transformers` version: 4.40.2\n",
            "- Platform: Linux-6.1.58+-x86_64-with-glibc2.35\n",
            "- Python version: 3.10.12\n",
            "- Huggingface_hub version: 0.23.0\n",
            "- Safetensors version: 0.4.3\n",
            "- Accelerate version: 0.30.1\n",
            "- Accelerate config: \tnot found\n",
            "- PyTorch version (GPU?): 2.2.1+cu121 (True)\n",
            "- Tensorflow version (GPU?): 2.15.0 (True)\n",
            "- Flax version (CPU?/GPU?/TPU?): 0.8.3 (gpu)\n",
            "- Jax version: 0.4.26\n",
            "- JaxLib version: 0.4.26\n",
            "- Using GPU in script?: <fill in>\n",
            "- Using distributed or parallel set-up in script?: <fill in>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!transformers-cli env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZ0EMjVOgvHr"
      },
      "outputs": [],
      "source": [
        "## Unsloth install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Ljmnub2mndD"
      },
      "outputs": [],
      "source": [
        "# # # Empty VRAM\n",
        "# del model\n",
        "\n",
        "# # del trainer\n",
        "# import gc\n",
        "# gc.collect()\n",
        "# gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwXcOFXfg-r_"
      },
      "source": [
        "## Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLVIFd9lhGNE"
      },
      "outputs": [],
      "source": [
        "# For gated models on HuggingFace\n",
        "# from huggingface_hub import notebook_login\n",
        "# notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrrQWjPmhRsr",
        "outputId": "841b8b3c-a504-488f-c976-5b33a209e46a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: HF_HUB_ENABLE_HF_TRANSFER=True # for high speed downloading and uploading to hugging face hub\n"
          ]
        }
      ],
      "source": [
        "%env HF_HUB_ENABLE_HF_TRANSFER = True # for high speed downloading and uploading to hugging face hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knZB6c-phmPB"
      },
      "outputs": [],
      "source": [
        "cache_dir = '' # comment out if Google Drive is aset as cache_dir\n",
        "\n",
        "# base model (Unsupervised Trial)\n",
        "model_id = \"openchat/openchat_3.5\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7WvkXooiRhB"
      },
      "outputs": [],
      "source": [
        "## Load the model and Tokenizer of LoRA or DoRA\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit = True,\n",
        "    bnb_4bit_use_double_quant = True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16, # if newer gpu: bfloat16\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TDi7OpEfYcq"
      },
      "outputs": [],
      "source": [
        "# config = AutoConfig.from_pretrained(model_id)\n",
        "# cofig.max_position_embeddings = 4096 # (input + output) #model will only learn from max 4096 sequence of token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WyrKD_WhXFn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515,
          "referenced_widgets": [
            "3b24b7c214d945d993fe3d1bf3220055",
            "53939441e9ed45b8ac8b4c044d69fbad",
            "c8a48c70e806453797a8054ac595c0a5",
            "cc141283ecd743fca3770b9d75e2ec64",
            "f60d788b397f41929b50b540071bb7d3",
            "c476d8b0072f4b90a6a8716ed1717be4",
            "402af069cffa45029e80bddc81cd480c",
            "0be72480a53a4a8f9c50e1e568a13660",
            "8f3753fc47eb445d8dd9da6dee5e53e0",
            "65cd6bcd49494913a8634541b6325ccb",
            "f67a1968471a4b52829bf97fdfc8ee9c",
            "df96aa180fb04bdca5ec40c2116f7ecd",
            "a895d146b3a44ec6997ef1bb18ab9ee0",
            "27963145d7d945a79edd012e035f6cf6",
            "8f50363525d54ee8b0aff95f1d9f15ea",
            "3e00ce454bb24094998c0f39f221586d",
            "12f9f1a925fc4df1aea223182b226e9a",
            "b706408b4a84446f88a14348358b46b9",
            "68c0bd99afe143cc9af1f05615970c6a",
            "e8139bcca5624c92922e3ed1796644f6",
            "a9b24a42acb64252b1cb931928d79847",
            "4c137cef3f54466485fd19ac36556427",
            "1e9ae97e2a264d4c8837f63a1185bc83",
            "e9141da0cc744116b10450f6a19e4499",
            "f00e5a643db1464d8354b1830edb4e81",
            "29a9b933be094bf69755401b9bdb9888",
            "0cda14f82c5b4c909fac685557c21e87",
            "155cd2978657486ead4934fba196c5a1",
            "f4633e092b6d46c097dd71aab340c88a",
            "87824907ddff458088a0725445da6314",
            "523e11704da741a7b9ae37cd4d1f4b08",
            "af36a6abe5364691858771a5def8d325",
            "ae866f2746ca4533baba02f0ceeb29d0",
            "413564de5bcf47f1aa33c268512e9880",
            "54ef087f0fa74df696a6a44cfa0210b4",
            "93b263ef247f485384a64a59516cc799",
            "b8a4514f7ca04368a75ae84cfe196b3f",
            "5ce6f3e48feb4bf09c57080c16a3d36a",
            "d0d4bb0246214d7fb3f8e98dc0b03750",
            "ba2f095fb2284a80937f0cb45be8d27c",
            "9c29e421c5374f46a3bb370d86a8fcf9",
            "54a5ae0b47674ccb8d941d55329c9409",
            "21713a5b87054ec99185c38cbbbfa923",
            "081dd2b1dd5040b3a50dcc32a70ee9db",
            "2e36fa92f38243c0a60705c89b754fdf",
            "9c8bd7c87d03420eb1c81a06b36286ae",
            "69b633ceed6e4a4e9b1bbbc96e79f463",
            "f4fc02714a824bb98777120dd9e46cb9",
            "bdc9166772bd4cdebc1eba871b494a23",
            "d9f08e848be348c2bbca6f5463159fe3",
            "52deae387683469c83744626a6d3fc7b",
            "f7433b7324e04147aa99939a5fe79f8f",
            "d290c65a264f409e9afb4db3b5bd8621",
            "888dae8a058842fcace3d9632584ca2b",
            "5d27d7a5811d4f4eb395e25d46db309b",
            "6a8d0747e44d4f28a8b2ba4c3081e742",
            "7b7ec0351c7747999c666168ae5c17ca",
            "2f4da8c8383740a0a230d124114d5ba3",
            "d30faef508d64e3da0c5910ac2a509f0",
            "b44fb322d90542e8b353bfb2251e7d9d",
            "7e72a0e18eee47ab80528b0ad4557273",
            "44ec631c967f416b951154dc1956054a",
            "abc2292fddff4dea9898ac8611e51eb5",
            "8d6837f38a9e4f4f9f3b968fecde0092",
            "05844be5cacc49879429d80ae3f4baaf",
            "3056dd7a706a473185e7791cdb10c8e0",
            "ccf01ea6a1a043379467f55811e40b5d",
            "99ebdffddf5146e991e26f40752fb212",
            "15d719f686e345cc9cee6463ae9cbe0d",
            "7a1b25bdbbaa49199bc5d5477774cf50",
            "5c370c3d934d4d448bb0241a1fe41c72",
            "12dfbfd0913d4432834f0724e0072b5f",
            "bb423bf454c14e30a228eed891adfe03",
            "620782b90bce401ba445a64b9e02ee4b",
            "f6759ee2461d46ae8c342ba802a71f9d",
            "beeb061e118a4575af194fd16cd3bca6",
            "2e954e2fdcdd4221b67394faa7725f1d"
          ]
        },
        "outputId": "1a9af9ca-342d-40dd-df10-d759d9dad08a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/623 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3b24b7c214d945d993fe3d1bf3220055"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "df96aa180fb04bdca5ec40c2116f7ecd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1e9ae97e2a264d4c8837f63a1185bc83"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "413564de5bcf47f1aa33c268512e9880"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model-00002-of-00002.bin:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2e36fa92f38243c0a60705c89b754fdf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a8d0747e44d4f28a8b2ba4c3081e742"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/179 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ccf01ea6a1a043379467f55811e40b5d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    #config=config,\n",
        "\n",
        "    quantization_config=bnb_config,\n",
        "\n",
        "    #rope_scaling={\"type\":linear, \"factor\": 2.0}, # roPE scaling: https://www.hopsworks.ai/dictionary/rope-scaling and https://huggingface.co/docs/text-generation-inference/en/basic_tutorials/preparing_model\n",
        "\n",
        "    #https://huggingface.co/docs/accelerate/v0.25.0/en/concept_guides/big_model_inference\n",
        "    # device_map='auto', # It’s fully possible to create your own device map for the layers to use as well, specifying the GPU device to use (a number), \"cpu\", or \"disk\" and pass this in:\n",
        "    device_map = {\"\": 0}, # above auto wasnot working\n",
        "\n",
        "    # Here, the \"trust_remote_code=True\" means \"download the model code from huggingface repo 'internlm/internlm-chat-7b'\", along with the weight, and run it. If it's False, the library would use builtin model architectures hardcoded in huggingface/transformers and only download the weight.\n",
        "    #trust_remote_code=False,\n",
        "\n",
        "    torch_dtype=torch.float16, # if newer gpu: bfloat16\n",
        "\n",
        "    # https://huggingface.co/docs/text-generation-inference/en/conceptual/flash_attention\n",
        "    # attn_implementation=\"flash_attention_2\", # Works with llama model\n",
        "\n",
        "    cache_dir = cache_dir\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rS-hXeLwoclN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195,
          "referenced_widgets": [
            "14e00778f19d424da3232c27fda53660",
            "91a0f16906964c53b19c77bc67748036",
            "91be794e1d3c4517b396383697395523",
            "45249481e4ae4983b0d2fa28d1c686e9",
            "37fd91c7c1db411d9804d8c318a982c9",
            "456b7bfb442e42d99f452dee2ae53c9e",
            "4c7974d57ede4b36b54480e899f49742",
            "ece608eb6cfa4eb0a6e51bb52b6615e9",
            "a890d0a8d58f4257af7b86b2069b5609",
            "5ce49c7d00324bf39f71be58dda66dd9",
            "e96a8f6080e64d37ace1c624f09d5ea2",
            "b73989e354254d22a6da14841f6900a2",
            "23cf23cccc294a598f985c125c33c9ab",
            "b49ac111521d4e90bd71f80e79f5e6e5",
            "54edf31467874335a0744dd83bf36d24",
            "5c674a033a7c460a88e97da84343c3e4",
            "3cffbb54ad9f4d9194fe9718ea2f7b30",
            "cfb4b47e3fdf45aa83f78f040cc95503",
            "9c4501e42c3c4546b4aedafe9510aab2",
            "b662d7d8463c4f168eb6d8de8e31e6a1",
            "96bc88ea5f7b4350b780e2719d9b601d",
            "8d3e39b135fb4b39b0eac8608f49694d",
            "d6390b7299b84a5db8ffa659c6b4c752",
            "17b768f671d44a52b9cec8257c7cfc45",
            "fe47202c3676463b92b9118d03dd690e",
            "6c7bb6818ee3478f9baee4ba7c28cf17",
            "ddc6019662364fa1b2e08d7ba8beb6ff",
            "d55241b77b544b2ebc82b73f8925e051",
            "e685f4f82c1a459c84e98505a3da97b2",
            "8c25d22ef55b4f8e97c3a83537626503",
            "a5e4c47ec0d44f70be801f32b8927a26",
            "429a5b20d10545e4b7614ddac7fba81d",
            "819edddad15f4e64b59d32c2541dd95f",
            "234b76c86367440c97da2eddd9451b0c",
            "a9c7bf087ed94853ab260514ba0e3b1f",
            "9c708a7c78d54954b2e23618d2fff7ab",
            "2a6f3c4eff37415a9c3d4594bccca035",
            "01d0df3e29544f33815cf8749f6bc365",
            "9ea5f34d21324131969875f9f5327043",
            "520beea460c442b9b91961c55ed588b5",
            "4275239ddfdc4578883383f4e905be90",
            "94d4c846f7da4e2fa3630f965b2056ad",
            "44b9092e6f234201859188dd9977087e",
            "b7764f35a5bc4922be96135e725ce5fb",
            "130db93ef8d74e1cbd57caca1cde6e6c",
            "f6bf8a9021734f71b0783a7fe4c6d862",
            "b284364f8ea142498c1aa3d8965d9be1",
            "1d1872db82a2424884140ff5c274b163",
            "c6a1f3a72a9446e2a17e8d6012379cb2",
            "c3407bcab45b4bdb91cbe1bbcea9c36c",
            "23a76f25367d4973a67777e0fbd1b5d2",
            "7e31fcbd9ca04bb4b04db668db774921",
            "c3daa72099dd4741b0001ce7ae4520fa",
            "c04a526b39ae4cf9b2c39974434b2d98",
            "63f4b5b7281e4aea97871de1099a7929"
          ]
        },
        "outputId": "4a6b03ee-f5aa-4aaa-c9a2-f7c0ea41ab9d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "14e00778f19d424da3232c27fda53660"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b73989e354254d22a6da14841f6900a2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d6390b7299b84a5db8ffa659c6b4c752"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "234b76c86367440c97da2eddd9451b0c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/491 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "130db93ef8d74e1cbd57caca1cde6e6c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True, trust_remote_code=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkm8areWsylC"
      },
      "outputs": [],
      "source": [
        "## Load the Model and Tokenizer for Unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tdt9Gof7s39U"
      },
      "source": [
        "## Loading checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6h6k1m5s7yT"
      },
      "outputs": [],
      "source": [
        "# Check there are no parameter overflowing onto cpu (meta)\n",
        "# Making sure all of the parameter are in GPU not in CPU\n",
        "for n, p in model.named_parameters():\n",
        "  if p.device.type == \"meta\":\n",
        "    print(f\"{n} is on meta\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrO5Z-q8tKbo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fcae068-eaa8-46fe-dea3-e6321fce1714"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8192\n",
            "32000\n"
          ]
        }
      ],
      "source": [
        "print(model.config.max_position_embeddings)\n",
        "\n",
        "#eos = end of sequence\n",
        "# https://huggingface.co/docs/transformers/en/pad_truncation\n",
        "# very important for pad and eos use: https://www.natebrake.com/blog/llm/end-of-sequence-explained\n",
        "print(model.config.eos_token_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TkXPSLszUdCy"
      },
      "outputs": [],
      "source": [
        "## Prepare for LoRA fine-tuning\n",
        "def print_trainable_parameters(model):\n",
        "  \"\"\"\n",
        "  Print the number of trainable parameters in the model and lists whic\n",
        "  \"\"\"\n",
        "  trainable_params = 0\n",
        "  non_trainable_params = 0\n",
        "  all_params = 0\n",
        "\n",
        "  print(\"Trainable Parameters:\")\n",
        "  for name, param in model.named_parameters():\n",
        "    # https://www.geeksforgeeks.org/python-pytorch-numel-method/\n",
        "    # Total no of all parameters (trainable + non trainable)\n",
        "    all_params += param.numel() #PyTorch torch.numel() method returns the total number of elements in the input tensor.\n",
        "\n",
        "    # source: copilot: ask about param.requires_drad\n",
        "    # When requires_grad is set to True, it indicates that the parameter participates in gradient computation during backpropagation (i.e., it’s trainable).\n",
        "    #When requires_grad is set to False, the parameter is excluded from gradient updates during training (i.e., it’s frozen).\n",
        "    if param.requires_grad:\n",
        "      trainable_params += param.numel()\n",
        "      print(f\"  {name} \")\n",
        "    else:\n",
        "      non_trainable_params += param.numel()\n",
        "\n",
        "  # This part is same as else portion above but just for printing we did it again\n",
        "  print(\"\\nNon_Trainable Parameters\")\n",
        "  for name, param in model.named_parameters():\n",
        "    if not param.requires_grad:\n",
        "      print(f\" {name} \")\n",
        "\n",
        "\n",
        "  print(\n",
        "      f\"\\nSummary:\\n Trainable params: {trainable_params}\\n Non-Trainable params:{non_trainable_params}\"\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3P2OKHsYsx1"
      },
      "source": [
        "## Standard LoRA or DoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OeUh4M73YziD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6d18a65-b0a8-4ed1-d43e-6af368484e43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MistralForCausalLM(\n",
            "  (model): MistralModel(\n",
            "    (embed_tokens): Embedding(32002, 4096)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x MistralDecoderLayer(\n",
            "        (self_attn): MistralSdpaAttention(\n",
            "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (rotary_emb): MistralRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): MistralMLP(\n",
            "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): MistralRMSNorm()\n",
            "        (post_attention_layernorm): MistralRMSNorm()\n",
            "      )\n",
            "    )\n",
            "    (norm): MistralRMSNorm()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4o55adu3aAjj"
      },
      "source": [
        "Important documentaion for large model faster training.\n",
        "\n",
        "https://huggingface.co/docs/transformers/v4.18.0/en/performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqZa78QOZaOO"
      },
      "outputs": [],
      "source": [
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "# look at gradient checkpointing and gradient accumulation on https://huggingface.co/docs/transformers/v4.18.0/en/performance\n",
        "model.gradient_checkpointing_enable() # to save some memory in VRAM in turn for little slow training\n",
        "\n",
        "model = prepare_model_for_kbit_training(model) # for quantization, must be uncommented.\n",
        "\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# Understaing Lora parameters: https://medium.com/@drishtisharma96505/comparative-analysis-of-lora-parameters-on-llama-2-with-flash-attention-574b913295d4\n",
        "peft_config = LoraConfig( #matching the Llama recipe\n",
        "                         r = 8,\n",
        "                          lora_alpha = 32,\n",
        "                          target_modules = [\n",
        "                              \"q_proj\",\n",
        "                              \"k_proj\",\n",
        "                              \"v_proj\",\n",
        "                              \"o_proj\",\n",
        "                              # \"self_attn.rotary_emb.inv_freq\",\n",
        "\n",
        "                              ## comment out 3 below for mixtril\n",
        "                              \"gate_proj\",\n",
        "                              \"up_proj\",\n",
        "                              \"down_proj\",\n",
        "\n",
        "                              # \"lora_magnitude_vector\" # required for DoRA,\n",
        "                              # \"input_layernorm.weight\",\n",
        "                              # \"post_attention_layernorm.weight\",\n",
        "                              # \"model.norm.weight\",\n",
        "                              # \"lm_head.weight\",\n",
        "\n",
        "\n",
        "                              # \"dense_h_to_4h\",  #for falcon\n",
        "                              # \"dense_4h_to_h\",  #for falcon\n",
        "                              # \"query_key_value\",  #for falcon\n",
        "                              # \"dense\" #for falcon\n",
        "                          ],\n",
        "                          lora_dropout = 0.1,\n",
        "                          bias = \"none\",\n",
        "                          task_type=\"CAUSAL_LM\"\n",
        "                          )\n",
        "\n",
        "model = get_peft_model(model, peft_config) #move to a peft model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vb5daNF7pHHH"
      },
      "outputs": [],
      "source": [
        "# print_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUJ8ucWJpTYm"
      },
      "outputs": [],
      "source": [
        "## Unsloth LoRA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNVdqLOOpWaZ"
      },
      "source": [
        "## Set up Tokenizer and Padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVWJ5dAVpds0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9146137d-b8bd-44d3-aea6-be7d6d031860"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LlamaTokenizerFast(name_or_path='openchat/openchat_3.5', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '<|end_of_turn|>', 'unk_token': '<unk>', 'additional_special_tokens': ['<|end_of_turn|>', '<|pad_0|>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
            "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32000: AddedToken(\"<|end_of_turn|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32001: AddedToken(\"<|pad_0|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "}\n",
            "32000\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer)\n",
        "print(tokenizer.vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GXSr5XBprg1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbaa3a33-9dbf-4540-ac53-5d56f34c92be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>\n",
            "<|end_of_turn|>\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.bos_token) #check begining of sequence\n",
        "print(tokenizer.eos_token) # end of sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1IfC82bYqIS7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "432aae60-a00f-429b-96c1-677ba10c9040"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>GPT4 Correct User: write a quick sort algorithm in python<|end_of_turn|>GPT4 Correct Assistant: here your are<|end_of_turn|>GPT4 Correct User: great.<|end_of_turn|>\n"
          ]
        }
      ],
      "source": [
        "# # Optionally set the chat template manually.\n",
        "# tokenizer.chat_template = \"{ if not add_generation_prompt is defined %}\"\n",
        "\n",
        "# Test the chat template\n",
        "messages = [\n",
        "    {'role': 'user', 'content': \"write a quick sort algorithm in python\"},\n",
        "    {'role': 'assistant', 'content': \"here your are\"},\n",
        "    {'role': 'user', 'content':\"great.\"}\n",
        "]\n",
        "\n",
        "# When you set tokenize=False in the tokenizer.apply_chat_template() function, it means that the resulting chat template output will not be tokenized into individual tokens. Instead, it remains as a single string without any tokenization. This can be useful when you want to keep the entire chat history intact for further processing or analysis. 😊\n",
        "inputs = tokenizer.apply_chat_template(messages, tokenize = False)\n",
        "print(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wTTjGDBrp2q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "011923a3-4aca-48d0-901f-7f0c9124e924"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<unk> token is in the tokenizer. Using unk for pad\n"
          ]
        }
      ],
      "source": [
        "# very important for pad and eos use: https://www.natebrake.com/blog/llm/end-of-sequence-explained\n",
        "# Choosing pad_token for tokenizer\n",
        "\n",
        "## Option A - set the pad token to <pad>, if not <|pad|>, if not <unk> if\n",
        "if '<pad>' in tokenizer.get_vocab():\n",
        "  print('<pad> token is in the tokenizer. Using <pad> for pad')\n",
        "  #set the pad token\n",
        "  tokenizer.pad_token = '<pad>'\n",
        "elif '<|pad|>' in tokenizer.get_vocab():\n",
        "  print('<|pad|> token is in the tokenizer. Using <|pad|> for pad')\n",
        "  #set the pad token\n",
        "  tokenizer.pad_token = '<|pad|>'\n",
        "elif '<unk>' in tokenizer.get_vocab():\n",
        "  print('<unk> token is in the tokenizer. Using unk for pad')\n",
        "  # Set the pad token\n",
        "  tokenizer.pad_token = '<unk>'\n",
        "else: # choosing eos_token as pad_token may be risky.\n",
        "  print(f'Using EOS token, {tokenizer.eos_token}, for padding.')\n",
        "  tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "# ## OPTION B - create pad token\n",
        "# # Check if the pad token is already in the tokenizer vocabulary\n",
        "# if '<pad>' not in tokenizer.get_vocab():\n",
        "#   print('pad token not in the tokenizer, adding a <pad> token')\n",
        "\n",
        "#   #Add the pad token\n",
        "#   tokenizer.add_tokens(['<pad>'])\n",
        "#   # set the pad token\n",
        "#   tokenizer.pad_token = '<pad>'\n",
        "#   # Resize token embeddings\n",
        "#   model.resize_token_embeddings(tokenizer.vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdRrD-Q-vPgq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05ca6fb6-d488-44d3-9ac5-eead37b8d86d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer pad token ID: 0\n",
            "Model pad token ID: 0\n",
            "Model config pad token ID: 0\n",
            "Number of tokens now in tokenizer: 32000\n"
          ]
        }
      ],
      "source": [
        "# Update pad token id in model and it's config\n",
        "model.pad_token_id = tokenizer.pad_token_id\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# Check if they are equal\n",
        "assert model.pad_token_id == tokenizer.pad_token_id\n",
        "\n",
        "# Print the pad token ids\n",
        "print('Tokenizer pad token ID:', tokenizer.pad_token_id)\n",
        "print('Model pad token ID:', model.pad_token_id)\n",
        "print('Model config pad token ID:', model.config.pad_token_id)\n",
        "print('Number of tokens now in tokenizer:', tokenizer.vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sXi7USFFU9-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d210ad56-64d7-4e5b-ca3a-adb4e9f7d483"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Special tokens map: {'bos_token': '<s>', 'eos_token': '<|end_of_turn|>', 'unk_token': '<unk>', 'pad_token': '<unk>', 'additional_special_tokens': ['<|end_of_turn|>', '<|pad_0|>']}\n",
            "All special tokens: ['<s>', '<|end_of_turn|>', '<unk>', '<|pad_0|>']\n"
          ]
        }
      ],
      "source": [
        "print(\"Special tokens map:\", tokenizer.special_tokens_map)\n",
        "print( \"All special tokens:\", tokenizer.all_special_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giXHhFbWWjT-"
      },
      "outputs": [],
      "source": [
        "tokenizer.padding_side = 'right'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uYpoZjyLDq5"
      },
      "outputs": [],
      "source": [
        "# # Uncomment to switch to left padding, not recommended for unsloth\n",
        "# tokenizer.padding_side = 'left # left padding is ususally not good idea for most model, but some use cases it may be useful"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_vNLYPrLsWB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cb3745f-22c7-42e0-f911-027fe2a1e86e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LlamaTokenizerFast(name_or_path='openchat/openchat_3.5', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '<|end_of_turn|>', 'unk_token': '<unk>', 'pad_token': '<unk>', 'additional_special_tokens': ['<|end_of_turn|>', '<|pad_0|>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
            "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32000: AddedToken(\"<|end_of_turn|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32001: AddedToken(\"<|pad_0|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIJdUBPCMBii"
      },
      "source": [
        "## Set embed and norms layers to trainable (recommended only for chat fine tuning if your changing the template or changing the context length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlTGveezMJG2"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ObmdhqKMWWR"
      },
      "source": [
        "## Set up Evaluation\n",
        "\n",
        "- optional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Bw2hJQKMYyT"
      },
      "outputs": [],
      "source": [
        "from transformers import TextStreamer\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "import gc  # import Python's garbage collection module\n",
        "\n",
        "# Define a stream\n",
        "def Stream(user_prompt, model_type, tokenizer, checkpoint=''):\n",
        "\n",
        "  if model_type == 'base':\n",
        "    eval_model = model\n",
        "  elif model_type == 'fine-tuned':\n",
        "    eval_model = PeftModel.from_pretrained(model, checkpoint)\n",
        "    eval_model = eval_model.to(\"cuda\") # compute in GPU\n",
        "\n",
        "    for n, p in eval_model.named_parameters():\n",
        "      if p.device.type == \"cpu\":\n",
        "        print(f\"{n} is on CPU!\")\n",
        "  else:\n",
        "    print(\"You must set the model_type to base or fine-tuned\")\n",
        "\n",
        "  # print (f'Proceeding to inference with peft adapters from {checkpoint}')\n",
        "\n",
        "  # Source: chatgpt: model.config.use_cache = True\n",
        "  # The use_cache option allows the model to cache intermediate hidden states and attention weights as it generates tokens.\n",
        "  # This cache helps speed up subsequent token generation by reusing previously computed information.\n",
        "  # If you’re generating long sequences or performing autoregressive tasks (where each token depends on previous tokens), enabling cache can significantly improve decoding speed.\n",
        "  eval_model.config.use_cache = True\n",
        "\n",
        "  messages = [\n",
        "      # strip() returns new string with extra(unwanted) white space removed\n",
        "      {'role': 'user', 'content': f\"{user_prompt.strip()}\",}\n",
        "  ]\n",
        "\n",
        "  # add generation prompt must be true for giving ai where to start it's geneartion from in chat prompt eg: this will add at the end: <|im_start|>assistant\n",
        "  inputs = tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt=True)\n",
        "\n",
        "  # \"pt\" means pytorch tensors When you set return_tensors=\"pt\", the tokenizer or model returns the tokenized input as PyTorch tensors.\n",
        "  #These tensors can be directly used for model inference or fine-tuning.\n",
        "  inputs = tokenizer([inputs], return_tensors=\"pt\", add_special_tokens=False)\n",
        "\n",
        "  # there will be token_type_ids in the end of prompt like Token Type IDs: [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
        "  # these are useful for many places but not here.\n",
        "  if \"token_type_ids\" in inputs: # we don't token_type_ids here\n",
        "    del inputs[\"token_type_ids\"]\n",
        "\n",
        "  streamer = TextStreamer(tokenizer)\n",
        "\n",
        "  print(f'eval_model is on:',{next(eval_model.parameters()).device}) # CPu or CUDA\n",
        "  print(f'input_ids are on: {inputs[\"input_ids\"].device}')\n",
        "\n",
        "  # parameter of .generate: https://huggingface.co/docs/transformers/en/main_classes/text_generation\n",
        "  _ = eval_model.generate(**inputs, streamer=streamer, max_new_tokens=10, use_cache=True)\n",
        "\n",
        "  # Clear GPU cache and run garbage collection\n",
        "  torch.cuda.empty_cache() # Clear GPU cache\n",
        "  gc.collect() # Run garbage collection\n",
        "\n",
        "def evaluation(model_type, tokenizer, checkpoint=''):\n",
        "  questions = [\n",
        "    \"In the context of Touch Rugby Internation Rules 2020, what does the dead ball line marks?\",\n",
        "    \"How many players are on the field on each team in touch rugby?\",\n",
        "    \"In touch rugby, does a forward pass result in a roll ball or a Penalty\",\n",
        "    \"In touch rughby, how long is half time?\"\n",
        "    \"In touch rugby, how does the game commence?\"\n",
        "    \"In touch rugby, how many points is a try worth?\"\n",
        "    \"\"\n",
        "  ]\n",
        "\n",
        "  answers = [\n",
        "      \" The Dead ball line marks the end boundaries of the field of play\",\n",
        "      \"6 players\",\n",
        "      \"Penalty\",\n",
        "      \"5 minutes\",\n",
        "      \"The game begins with a tap on the halfway line\"\n",
        "      \"1 point\"\n",
        "  ]\n",
        "\n",
        "  for question, answer in zip(questions, answers):\n",
        "    Stream(question, model_type, tokenizer, checkpoint)\n",
        "    print(\"Correct Answer:\", answer)\n",
        "    print('\\n\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYKKAu-GVPik",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a31b779-2a58-43d1-dc64-b3a6253b5900"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MistralConfig {\n",
            "  \"_name_or_path\": \"openchat/openchat_3.5\",\n",
            "  \"architectures\": [\n",
            "    \"MistralForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"mistral\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"float16\",\n",
            "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 4096,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.40.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32002\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(model.config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkztvnrVYm_X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ebbfb1a-3bbb-4d9f-b82d-657bbbd1d699"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"max_length\": 8192,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"temperature\": 0.5\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(model.generation_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmFGWFZZYquq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4434b912-954a-44a2-b7aa-f4c261eb27bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eval_model is on: {device(type='cuda', index=0)}\n",
            "input_ids are on: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> GPT4 Correct User: In the context of Touch Rugby Internation Rules 2020, what does the dead ball line marks?<|end_of_turn|> GPT4 Correct "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1510: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assistant: In Touch Rugby International Rules 202\n",
            "Correct Answer:  The Dead ball line marks the end boundaries of the field of play\n",
            "\n",
            "\n",
            "\n",
            "eval_model is on: {device(type='cuda', index=0)}\n",
            "input_ids are on: cpu\n",
            "<s> GPT4 Correct User: How many players are on the field on each team in touch rugby?<|end_of_turn|> GPT4 Correct "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assistant: In touch rugby, each team has 13\n",
            "Correct Answer: 6 players\n",
            "\n",
            "\n",
            "\n",
            "eval_model is on: {device(type='cuda', index=0)}\n",
            "input_ids are on: cpu\n",
            "<s> GPT4 Correct User: In touch rugby, does a forward pass result in a roll ball or a Penalty<|end_of_turn|> GPT4 Correct Assistant: In touch rugby, a forward pass results in a\n",
            "Correct Answer: Penalty\n",
            "\n",
            "\n",
            "\n",
            "eval_model is on: {device(type='cuda', index=0)}\n",
            "input_ids are on: cpu\n",
            "<s> GPT4 Correct User: In touch rughby, how long is half time?In touch rugby, how does the game commence?In touch rugby, how many points is a try worth?<|end_of_turn|> GPT4 Correct Assistant: In touch rugby, half time is typically 5\n",
            "Correct Answer: 5 minutes\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# checking the base model which hasnot been fine tunned\n",
        "evaluation(\"base\", tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18_D1lCTY772"
      },
      "source": [
        "## Load the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kc-OZ-BSZILm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "02a4b05857ee4c2280812bad19f4cc1c",
            "d18f23e3c0114597a60464c661d0cf73",
            "b28b830ac7cb4f5397ffda08db75a2b1",
            "8e37766044ff446fb00447dd8227a516",
            "01ebac5e28fe40f388e10b032d66d191",
            "6b8bcfe79e4d4df5a2c730eaf6a0290c",
            "c0bb64885ae54382ae5bd0bc321210d7",
            "e818bc29b2d9435383241450fdf83439",
            "5f1d33eb3e1446fd8fb92430b7cc7546",
            "ffbf6cc1ed84467793a993118ee9c6e8",
            "db0947de2fa947d494979bfcd099318c",
            "db303717191e4a5d847e1beba62c5510",
            "4548496ad0f44cf6bc39c27f3331199d",
            "dce830563b1e49268ef1d6b68a4d14d1",
            "5fdc21175cf848438768adac0e090953",
            "ad256305bd2a42aa881217629f344177",
            "3290a97831c44761a9500a15b37185ab",
            "09248066e5be486c9bb575df6a0abfca",
            "4fc2eb41eb464087af39da8debcbd008",
            "5cebf9cb4b164d6db8156891e972bbfb",
            "4d10e160f8e6486ca3b66048e53b6081",
            "f84cbf89198c401190714c4f319b3722",
            "15114b93f26b4c509e75ed4ff0a27feb",
            "26ec84c9a7a74b79b8b42988414d076d",
            "9f766c36b7c94730bd989b47656a645b",
            "d6bf752599f443f9b924eb1ed3a1f413",
            "bc3b2e5319584fb4a436eb139c2bf2c2",
            "8d300226126747849a89861dbfe7d307",
            "ae0e6fdec8d5444eb0396e45039eeaa7",
            "5c3137fe3e734ad69c37b2d83a74a118",
            "427b657661a84f4d89734462033fb7ef",
            "e3870f40459e480488ebf370d34716e2",
            "a87be065ce7d4fffacee2787a3287297",
            "f86ceb505af94fd3bf5683a3ed7df3e1",
            "2bac7e9cc7e54d8a8f140b5e457f5658",
            "9d076a54360c41b3a8f90868b3355053",
            "c54a2a39980e431bb0515601fa278935",
            "2bf131fca8aa4abcbfa32c02927165e4",
            "a57fd64452144b05941c7c8e96dde0ce",
            "052ccc3b12b34530862160895f924416",
            "07bfe3dcf7ba459b8deff28600718024",
            "2cfdcaecb0424f3b8c5d2bb02ce5c353",
            "974e986a32144c78990acb72fccbcf1f",
            "9241d58618624a4eb2146811f0f98efe"
          ]
        },
        "outputId": "956509ef-5d96-4396-c768-518d503c4e1f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/450k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "02a4b05857ee4c2280812bad19f4cc1c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/104k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "db303717191e4a5d847e1beba62c5510"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/303 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "15114b93f26b4c509e75ed4ff0a27feb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/60 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f86ceb505af94fd3bf5683a3ed7df3e1"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = 'Trelis/touch-rugby-rules-memorisation'\n",
        "\n",
        "data = load_dataset(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aI_Hfj1kZk4t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dad1e15b-08ac-4cbe-9cec-25961848e621"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First row of train: {'messages': '{\"messages\": [{\"role\": \"user\", \"content\": \"According to the copyright notice, who holds the rights to the Touch Football Rules?\"}, {\"role\": \"assistant\", \"content\": \"Touch Football Australia holds the rights to the Touch Football Rules as they are protected by copyright laws.\"}, {\"role\": \"user\", \"content\": \"What should players, coaches, and referees do if there are any adaptations or alterations to the rules for local competitions?\"}, {\"role\": \"assistant\", \"content\": \"Players, coaches, and referees should refer to relevant competition guidelines where any adaptations or alterations to the rules for local competitions are clearly articulated.\"}, {\"role\": \"user\", \"content\": \"What is the purpose of Appendix 1 in the document?\"}, {\"role\": \"assistant\", \"content\": \"Appendix 1 provides information about the Field of Play in the Touch Football rules.\"}, {\"role\": \"user\", \"content\": \"What is the term used to describe the period of time after an Infringement in which the non-offending side has the opportunity to gain an advantage either territorial, tactical, or in the form of a Try?\"}, {\"role\": \"assistant\", \"content\": \"Advantage\"}, {\"role\": \"user\", \"content\": \"What is the line on or over which a player has to place the ball to score a Try called?\"}, {\"role\": \"assistant\", \"content\": \"Attacking Try Line\"}]}'}\n",
            "First row of test: {'messages': '{\"messages\": [{\"role\": \"user\", \"content\": \"What was the process that led to the adoption of the 5th Edition FIT Playing Rules internationally?\"}, {\"role\": \"assistant\", \"content\": \"Touch Football Australia conducted an internal review of their domestic playing rules, which was then presented at the Federation of International Touch Paris Convention and subsequently adopted by FIT as the 5th Edition FIT Playing Rules.\"}, {\"role\": \"user\", \"content\": \"What rights did Touch Football Australia offer to FIT and all National Touch Associations regarding the use of the newly developed rules?\"}, {\"role\": \"assistant\", \"content\": \"Touch Football Australia offered FIT and all National Touch Associations royalty-free rights to use the newly developed rules presented in the TFA 8th Edition Playing Rules.\"}, {\"role\": \"user\", \"content\": \"What is the term used to describe the period after an infringement in which the non-offending side has the opportunity to gain an advantage?\"}, {\"role\": \"assistant\", \"content\": \"It is called \\\\\"Advantage\\\\\" in touch rugby terminology.\"}, {\"role\": \"user\", \"content\": \"What action is taken when a player receives three penalties from the defending team upon entering their Seven Metre Zone?\"}, {\"role\": \"assistant\", \"content\": \"The player is sent to the nearest Sin Bin Area and cannot be replaced or interchanged, which is known as \\\\\"Exclusion\\\\\" in touch rugby rules.\"}, {\"role\": \"user\", \"content\": \"What is the term used to describe the area in the Field of Play bounded by the Sidelines, the Try Lines, and the Dead Ball Lines?\"}, {\"role\": \"assistant\", \"content\": \"It is referred to as the In-Goal Area in touch rugby rules.\"}]}'}\n"
          ]
        }
      ],
      "source": [
        "# Print frist row of 'train and 'test'\n",
        "print(\"First row of train:\", data['train'][1])\n",
        "print(\"First row of test:\", data['test'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRlLorxbZ9Fk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "680f788c-174b-4f42-d39b-f5770024e7e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs: [1, 9830, 16167, 1264, 733, 6799, 9606, 1264, 345, 1838, 548, 345, 3789, 1264, 345, 3195, 12065, 302, 272, 25086, 13474, 5879, 403, 7567, 486, 25086, 13474, 6664, 438, 272, 22520, 302, 5440, 25086, 5465, 23717, 297, 4527, 28705, 28750, 28734, 28740, 28774, 1110, 881, 9830, 9606, 1264, 345, 489, 11143, 548, 345, 3789, 1264, 345, 1014, 320, 3120, 28705, 28783, 362, 20356, 6879, 288, 24759, 654, 7567, 486, 25086, 13474, 6664, 438, 272, 22520, 302, 5440, 25086, 5465, 23717, 297, 4527, 28705, 28750, 28734, 28740, 28774, 611, 881, 9830, 9606, 1264, 345, 1838, 548, 345, 3789, 1264, 345, 11447, 15813, 298, 8799, 272, 320, 3120, 28705, 28783, 362, 20356, 6879, 288, 24759, 390, 272, 28705, 28782, 362, 20356, 12630, 6879, 288, 24759, 17861, 578, 1110, 881, 9830, 9606, 1264, 345, 489, 11143, 548, 345, 3789, 1264, 345, 1014, 12630, 9217, 15813, 298, 8799, 272, 320, 3120, 28705, 28783, 362, 20356, 6879, 288, 24759, 390, 272, 28705, 28782, 362, 20356, 12630, 6879, 288, 24759, 298, 347, 1307, 2673, 544, 6157, 302, 272, 2039, 17861, 578, 611, 881, 9830, 9606, 1264, 345, 1838, 548, 345, 3789, 1264, 345, 5985, 3610, 25086, 6097, 697, 325, 8124, 2198, 28731, 3030, 298, 2136, 712, 28724, 23771, 298, 938, 272, 12486, 6202, 5879, 7567, 486, 25086, 13474, 6664, 1110, 881, 9830, 9606, 1264, 345, 489, 11143, 548, 345, 3789, 1264, 345, 2501, 28725, 25086, 13474, 6664, 6373, 12630, 304, 544, 3610, 25086, 6097, 697, 325, 8124, 2198, 28731, 13753, 884, 28733, 3669, 4495, 298, 938, 272, 12486, 6202, 5879, 611, 881, 9830, 9606, 1264, 345, 1838, 548, 345, 3789, 1264, 345, 3195, 349, 272, 9545, 302, 23074, 297, 272, 4993, 302, 272, 5879, 302, 272, 2039, 4771, 298, 12630, 1110, 881, 9830, 9606, 1264, 345, 489, 11143, 548, 345, 3789, 1264, 345, 23475, 23074, 297, 272, 4993, 302, 272, 5879, 302, 272, 2039, 349, 2278, 28725, 12630, 6191, 1291, 871, 3338, 298, 2405, 4190, 297, 1862, 8995, 5879, 298, 5407, 369, 544, 12850, 3555, 264, 1486, 28733, 14817, 2659, 611, 881, 9830, 9606, 1264, 345, 1838, 548, 345, 3789, 1264, 345, 6325, 26577, 2198, 442, 652, 23243, 8995, 14314, 506, 1581, 2918, 4331, 6731, 298, 272, 12630, 6879, 288, 24759, 1110, 881, 9830, 9606, 1264, 345, 489, 11143, 548, 345, 3789, 1264, 345, 5613, 28725, 707, 25661, 302, 442, 13496, 697, 298, 272, 24759, 354, 1862, 3440, 2065, 1023, 347, 6315, 22187, 6432, 297, 8598, 8995, 18392, 304, 347, 22058, 2632, 354, 5117, 28725, 25360, 28725, 304, 1792, 397, 274, 21892, 611, 28752, 9205]\n",
            "Decode Text: <s> {\"messages\": [{\"role\": \"user\", \"content\": \"What edition of the Touch Football rules was presented by Touch Football Australia at the Federation of International Touch Paris Convention in October 2019?\"}, {\"role\": \"assistant\", \"content\": \"The TFA 8th Edition Playing Rules were presented by Touch Football Australia at the Federation of International Touch Paris Convention in October 2019.\"}, {\"role\": \"user\", \"content\": \"Who resolved to adopt the TFA 8th Edition Playing Rules as the 5th Edition FIT Playing Rules internationally?\"}, {\"role\": \"assistant\", \"content\": \"The FIT Board resolved to adopt the TFA 8th Edition Playing Rules as the 5th Edition FIT Playing Rules to be used across all levels of the game internationally.\"}, {\"role\": \"user\", \"content\": \"Are National Touch Associations (NTAs) required to pay royalties to use the newly developed rules presented by Touch Football Australia?\"}, {\"role\": \"assistant\", \"content\": \"No, Touch Football Australia offered FIT and all National Touch Associations (NTAs) royalty-free rights to use the newly developed rules.\"}, {\"role\": \"user\", \"content\": \"What is the importance of consistency in the application of the rules of the game according to FIT?\"}, {\"role\": \"assistant\", \"content\": \"While consistency in the application of the rules of the game is important, FIT encourages its members to offer features in local competition rules to ensure that all participants enjoy a high-quality experience.\"}, {\"role\": \"user\", \"content\": \"Can NTAs or their authorized competition providers have different match conditions compared to the FIT Playing Rules?\"}, {\"role\": \"assistant\", \"content\": \"Yes, any adaptation of or alterations to the Rules for local competitions should be clearly articulated in relevant competition guidelines and be readily available for players, coaches, and referees alike.\"}]}\n"
          ]
        }
      ],
      "source": [
        "# Extract text from the first row of 'test' in data\n",
        "text = data['train'][0]['messages']\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = tokenizer.encode(text, add_special_tokens = True)\n",
        "\n",
        "# Decode back to text\n",
        "decoded_text = tokenizer.decode(tokens)\n",
        "\n",
        "# Print the tokens and decode text\n",
        "print(\"Token IDs:\", tokens)\n",
        "print(\"Decode Text:\", decoded_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJvSyXmxakNn"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyDYGe3ckNLo"
      },
      "source": [
        "## Set up and run Training (with saving of data logs to Drive)\n",
        "\n",
        "using TRL trainer is recommended."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKV3n4AmkijZ"
      },
      "source": [
        "### TRL Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGS0hYR8krw3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ff88def-8c38-41e7-d1bc-8849f6aff570"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length_1_grad_accum_1_batch_size_touch_rugby-rules\n"
          ]
        }
      ],
      "source": [
        "model_name = model_id.split(\"/\")[-1]\n",
        "dataset_name = dataset.split(\"/\")[-1]\n",
        "\n",
        "#parameters\n",
        "epochs = 1 # 1 epochs is good enough here\n",
        "context_length = 512 # most of the time Q and A arenot longer than 512\n",
        "\n",
        "# backpropagation params\n",
        "grad_accum = 1 # virtually increase the batch size. Maynot affect VRam but increase Training time\n",
        "batch_size = 1 # just granuale update # smooth and less update for help in memorization\n",
        "\n",
        "fine_tune_tag = 'touch_rugby-rules'\n",
        "save_dir = f'./results/{model_name}_{dataset_name}_{epochs}_epochs_{context_length}_length_{grad_accum}_grad_accum_{batch_size}_batch_size_{fine_tune_tag}'\n",
        "print(save_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25dAmFIdlzPw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "b9975c15-a2b7-4d44-add9-aeaf96ec6406"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "f-string: unmatched '[' (<ipython-input-52-4df9bbb9223f>, line 14)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-52-4df9bbb9223f>\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    f.write(f\"Step: {state.global_step}, Training Loss: {logs[\"loss\"]}\")\u001b[0m\n\u001b[0m                                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m f-string: unmatched '['\n"
          ]
        }
      ],
      "source": [
        "# #Custom callback for just logging\n",
        "# import transformers\n",
        "# import os\n",
        "\n",
        "# # custom callback to log metrics\n",
        "# class LoggingCallback(transformers.TrainerCallback):\n",
        "#   def _init_(self, log_file_path):\n",
        "#     self.log_file_path = log_file_path\n",
        "#     self.save_dir = save_dir\n",
        "\n",
        "#   def on_log(self, args, state, control, model = None, **kwargs):\n",
        "#     with open(self.log_file_path, 'a') as f:\n",
        "#       if 'loss' in loss:\n",
        "#         f.write(f\"Step: {state.global_step}, Training Loss: {logs[\"loss\"]}\")\n",
        "#       if 'eval_loss' in loss:\n",
        "#         f.write(f\"Step: {state.global_step}, Eval Loss: {logs['eval_loss']}\")\n",
        "\n",
        "#       f.flush() # Force flush the buffered data to file\n",
        "\n",
        "#     # Check if the current step is a checkpoint step\n",
        "#     if state.global_step % int(args.save_steps) == 0:\n",
        "#       # Check if the last checkpoint path exists\n",
        "#       if state.best_model_checkpoint:\n",
        "#         checkpoint_dir = state.best_model_checkpoint\n",
        "#       else:\n",
        "#         # if not, construct the checkpoint directory path manually\n",
        "#         checkpoint_dir = os.path.join(args.output_dir, f\"checkpoint\")\n",
        "\n",
        "#       #Ensure the checkpoint directory exist\n",
        "#       os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "#       # Save trainable params in the checkpoint directory\n",
        "#       current_trainable_params = {n: p for n, p in model.named_parameters()}\n",
        "#       current_trainable_params_state_dict = {n:p.data for n, p in current_trainable_params}\n",
        "\n",
        "#       file_path = os.path.join(checkpoint_dir, \"trainable_params.bin\")\n",
        "#       torch.save(current_trainable_params_state_dict, file_path)\n",
        "\n",
        "# # log file path\n",
        "# log_file_path = os.path.join(cache_dir, \"training_logs.txt\")\n",
        "\n",
        "# # Creating an instance of custom callback class\n",
        "# logging_callback = LoggingCallback(log_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDPUEYLIlgBM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTWLWet1nvlK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135,
          "referenced_widgets": [
            "e8731b6b5e714616bc140b24a7245e43",
            "716039d0865d4f38b7b48d1fecb87b5f",
            "08047ae736b640bab25579bfa91f0dc7",
            "37fea634d6d140a4a29b546a06190e43",
            "5fb407aaa1664e8fbb9a45b5dfcddfaa",
            "8f7f8fabe22b48ad9c05589a0ddb85ec",
            "62844fb548ef41ea8ab085e3b23d052b",
            "c6eebdcdf74c4a31b987c7804b2dc5e0",
            "da7cb5ae5406450bb54afdf34e23ad9f",
            "1e100096193a46a0a62cfc97d4c081cf",
            "2838ef68785e4e3cb5b1603b79ce252c",
            "58afaef810fa4c419f4d28d55bcf015f",
            "80f39c526e8e4685a01a15df22532791",
            "35dd23b63fc04a14bed91319de019222",
            "2bee7e6c1f7c459cbd3b2a479f2691d8",
            "a37c052a78d245bbbd2926082674e46e",
            "d0a609809fbc4ce3ac4d8d6168e9e398",
            "cbb13eb1e46e4f4596bdad801856fe35",
            "9da2b79f736845efa057021bbf64bda6",
            "043a56e23755463aaba5f59cf779b52b",
            "29baaeae47734fb687e75bc8fce946f3",
            "152a002e7b32482e8fe32469d60ee9fd"
          ]
        },
        "outputId": "dff706f4-c0a2-4877-db3b-aff79e72ac6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/303 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e8731b6b5e714616bc140b24a7245e43"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/60 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "58afaef810fa4c419f4d28d55bcf015f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "max_steps is given, it will override any value given in num_train_epochs\n",
            "Using auto half precision backend\n"
          ]
        }
      ],
      "source": [
        "from transformers import Trainer\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    # peft_config = peft_config # not needed where as look at above we have already put peft config directly into model but we comment it out\n",
        "    dataset_text_field = \"messages\", # key fied = \"messages\" in dataset in key , value pair\n",
        "    max_seq_length = context_length, # max length of query\n",
        "    tokenizer = tokenizer,\n",
        "    model=model.to(\"cuda\"),\n",
        "    train_dataset = data[\"train\"],\n",
        "    eval_dataset = data[\"test\"],\n",
        "\n",
        "    # about all the parameters: https://huggingface.co/docs/transformers/en/main_classes/trainer\n",
        "    args = TrainingArguments(\n",
        "        max_steps =1, # comment this out after first time you run.\n",
        "        save_steps = 50, ### make sure to check this value is good for our data, The save_steps parameter specifies the number of training steps between consecutive model checkpoints.\n",
        "        num_train_epochs = epochs,\n",
        "        output_dir = save_dir,\n",
        "        evaluation_strategy = \"steps\", # evaluation is done in every eval_steps\n",
        "        do_eval= True,\n",
        "        eval_steps = 0.2,\n",
        "        per_device_eval_batch_size = batch_size,\n",
        "        per_device_train_batch_size = batch_size,\n",
        "        gradient_accumulation_steps = grad_accum,\n",
        "        log_level =\"debug\",\n",
        "        optim = \"paged_adamw_32bit\", # if quantization\n",
        "        fp16 = True, # for low end non_ampere Gpu\n",
        "        #bf16 = True, # for only ampere GPU\n",
        "        max_grad_norm = 0.3,#The max_grad_norm value represents the maximum allowed norm (magnitude) of the gradients during backpropagation.. By setting a maximum norm, you prevent gradients from becoming too large, which can lead to unstable training or divergence.\n",
        "\n",
        "        # from chatgpt\n",
        "        # here cosine will be game changer as it decrease validataion loss and against overfitting\n",
        "        lr_scheduler_type = \"cosine\", # follow cosine shaped curve. cosine shape curve make sure lr decrease ove steps.\n",
        "        hub_private_repo = False,\n",
        "\n",
        "        # from chatgpt\n",
        "        # Warmup is an initial phase where the learning rate gradually increases from a very small value to its regular value. (0 to lr)\n",
        "        # It helps stabilize training and allows the model to explore the loss landscape more effectively.\n",
        "        # If you set warmup_ratio = 0.03 and T_max = 1000, the warmup phase will last for the first 30 steps (3% of the total).\n",
        "        warmup_ratio = 0.03,\n",
        "        # optim = \"adamw_torch\", # commented for LoRA+, we are using lora so needed\n",
        "        learning_rate= 1e-4, # comment for LoRA +\n",
        "        report_to=\"tensorboard\",\n",
        "    ),\n",
        "    # ,callbacks = [logging_callback], # if custom callback created\n",
        "    # optimizers = (optimizer, None) # for only LoRA +\n",
        "    # neftune_noise_alpha = 5 # Add in noise embeddings to improve performance\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnHoGdl43Wc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "outputId": "135f73e2-6006-44d0-e786-5df10e9b9b7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Currently training with a batch size of: 1\n",
            "***** Running training *****\n",
            "  Num examples = 303\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 1\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 1\n",
            "  Number of trainable parameters = 20,971,520\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1/1 00:57, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.489708</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 60\n",
            "  Batch size = 1\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1, training_loss=1.4443329572677612, metrics={'train_runtime': 60.8159, 'train_samples_per_second': 0.016, 'train_steps_per_second': 0.016, 'total_flos': 13478799237120.0, 'train_loss': 1.4443329572677612, 'epoch': 0.0033003300330033004})"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "model.config.use_cache = False # for silencing warnings only\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ip1EPFGF3weu"
      },
      "source": [
        "# Plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6g2tvW24AZ6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a11ffdd4-6b82-444c-d62a-7e7c79de9f1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulmffrbF4DVe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "9bc05cb3-6ae1-4367-abe8-d8fb045e92af"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize list to hold training and evaluation losses and steps\n",
        "train_losses = []\n",
        "eval_losses = []\n",
        "train_steps = []\n",
        "eval_steps = []\n",
        "\n",
        "#Populate the list from log history\n",
        "#import pandas as pd\n",
        "# pd.DataFrame(trainer.state.log_history)\n",
        "for entry in trainer.state.log_history:\n",
        "  if 'loss' in entry:\n",
        "    train_losses.append(entry['loss'])\n",
        "    train_steps.append(entry['step'])\n",
        "  if 'eval_loss' in entry:\n",
        "    eval_losses.append(entry['eval_loss'])\n",
        "    eval_steps.append(entry['step'])\n",
        "\n",
        "# plot the losses\n",
        "plt.plot(train_steps, train_losses, label = 'Train Loss')\n",
        "plt.plot(eval_steps, eval_losses, label = 'eval Loss')\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ogPiUfFPwkl"
      },
      "source": [
        "# Evaluate After Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Swt_0r7VP4ae"
      },
      "outputs": [],
      "source": [
        "# # Can set to true for faster inference\n",
        "# model.config.use_cache = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZQt4gyzP_Xh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7273381b-5544-4313-cab4-af5b4f23d5a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eval_model is on: {device(type='cuda', index=0)}\n",
            "input_ids are on: cpu\n",
            "<s> GPT4 Correct User: In the context of Touch Rugby Internation Rules 2020, what does the dead ball line marks?<|end_of_turn|> GPT4 Correct "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1510: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assistant: In Touch Rugby International Rules 202\n",
            "Correct Answer:  The Dead ball line marks the end boundaries of the field of play\n",
            "\n",
            "\n",
            "\n",
            "eval_model is on: {device(type='cuda', index=0)}\n",
            "input_ids are on: cpu\n",
            "<s> GPT4 Correct User: How many players are on the field on each team in touch rugby?<|end_of_turn|> GPT4 Correct Assistant: In touch rugby, each team has 13\n",
            "Correct Answer: 6 players\n",
            "\n",
            "\n",
            "\n",
            "eval_model is on: {device(type='cuda', index=0)}\n",
            "input_ids are on: cpu\n",
            "<s> GPT4 Correct User: In touch rugby, does a forward pass result in a roll ball or a Penalty<|end_of_turn|> GPT4 Correct Assistant: In touch rugby, a forward pass results in a\n",
            "Correct Answer: Penalty\n",
            "\n",
            "\n",
            "\n",
            "eval_model is on: {device(type='cuda', index=0)}\n",
            "input_ids are on: cpu\n",
            "<s> GPT4 Correct User: In touch rughby, how long is half time?In touch rugby, how does the game commence?In touch rugby, how many points is a try worth?<|end_of_turn|> GPT4 Correct Assistant: In touch rugby, half time is typically 5\n",
            "Correct Answer: 5 minutes\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "evaluation(\"base\", tokenizer) # use this if trained using adapter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHLimdulQLzT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMcbIVDX4mrFGQcAI7OJr6R",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}